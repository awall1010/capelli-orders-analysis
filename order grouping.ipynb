{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4542d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the data from the file\n",
    "# file_path = 'shippingdates/Rush Soccer 12.29 - Details.csv'  # Replace with the actual file path\n",
    "# df = pd.read_csv(file_path)\n",
    "# print(df.columns)\n",
    "# print(df.dtypes)\n",
    "\n",
    "\n",
    "# # Convert the 'Date Created' and 'Shipping Date' to datetime format\n",
    "# df['Date Created'] = pd.to_datetime(df['Date Created'])\n",
    "# df['Shipping Date'] = pd.to_datetime(df['Shipping Date'])\n",
    "\n",
    "# # df.rename(columns={\" Order Quantity \": \"Order Quantity\", \" Shipped Quantity \": \"Shipped Quantity\", ' Unshipped Quantity ': 'Unshipped Quantity' })\n",
    "# # Strip whitespace from all column names\n",
    "# df.columns = df.columns.str.strip()\n",
    "\n",
    "# # Strip whitespace from specific columns if they contain string values\n",
    "# columns_to_strip = ['Order Quantity', 'Shipped Quantity', 'Unshipped Quantity']\n",
    "# df[columns_to_strip] = df[columns_to_strip].apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# print(df.dtypes)\n",
    "# print(df.head())\n",
    "# # print(df.columns)\n",
    "\n",
    "# # Group by 'Customer Reference' and aggregate accordingly\n",
    "# grouped_df = df.groupby(['Customer Reference', 'Club Name']).agg({\n",
    "#     'Date Created': 'min',        # Minimum of Date Created\n",
    "#     'Order Quantity': 'sum',      # Sum of Order Quantity\n",
    "#     'Shipped Quantity': 'sum',    # Sum of Shipped Quantity\n",
    "#     'Unshipped Quantity': 'sum',  # Sum of Unshipped Quantity\n",
    "#     'Shipping Date': 'max',       # Maximum of Shipping Date\n",
    "#     'Sales Order Header Status': 'last'  # Take the first status (assuming it's consistent)\n",
    "# }).reset_index()\n",
    "\n",
    "# # Save the result to a new file\n",
    "# output_file = 'aggregated_orders12.29.csv'  # Replace with the desired output file name\n",
    "# grouped_df.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f\"Aggregated data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c81b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Initial Columns: ['Customer Reference', 'Club Name', 'Date Created', 'Sold TO Name', 'Sold TO Email', 'Ship TO Name', 'Order Quantity', 'Shipped Quantity', 'Unshipped Quantity', 'Tracking Number', 'Sales Order Header Status', 'Material Code', 'Description', 'Size', 'Shipping Date']\n",
      "Initial Data Types:\n",
      " Customer Reference            int64\n",
      "Club Name                    object\n",
      "Date Created                 object\n",
      "Sold TO Name                 object\n",
      "Sold TO Email                object\n",
      "Ship TO Name                 object\n",
      "Order Quantity                int64\n",
      "Shipped Quantity              int64\n",
      "Unshipped Quantity            int64\n",
      "Tracking Number              object\n",
      "Sales Order Header Status    object\n",
      "Material Code                object\n",
      "Description                  object\n",
      "Size                         object\n",
      "Shipping Date                object\n",
      "dtype: object\n",
      "Data Types After Preprocessing:\n",
      " Customer Reference                    int64\n",
      "Club Name                            object\n",
      "Date Created                 datetime64[ns]\n",
      "Sold TO Name                         object\n",
      "Sold TO Email                        object\n",
      "Ship TO Name                         object\n",
      "Order Quantity                        int64\n",
      "Shipped Quantity                      int64\n",
      "Unshipped Quantity                    int64\n",
      "Tracking Number                      object\n",
      "Sales Order Header Status            object\n",
      "Material Code                        object\n",
      "Description                          object\n",
      "Size                                 object\n",
      "Shipping Date                datetime64[ns]\n",
      "dtype: object\n",
      "First 5 Rows After Preprocessing:\n",
      "    Customer Reference              Club Name Date Created Sold TO Name  \\\n",
      "0             1258889           Rush Alabama   2024-07-24        BELLA   \n",
      "1             1258891  Rush Select Academies   2024-07-24      DIMITRI   \n",
      "2             1258891  Rush Select Academies   2024-07-24      DIMITRI   \n",
      "3             1258891  Rush Select Academies   2024-07-24      DIMITRI   \n",
      "4             1258891  Rush Select Academies   2024-07-24      DIMITRI   \n",
      "\n",
      "                     Sold TO Email Ship TO Name  Order Quantity  \\\n",
      "0  blake.oliver@inspeclabeling.com       OLIVER               2   \n",
      "1       zuckermanheather@yahoo.com        LOUIS               1   \n",
      "2       zuckermanheather@yahoo.com        LOUIS               1   \n",
      "3       zuckermanheather@yahoo.com        LOUIS               1   \n",
      "4       zuckermanheather@yahoo.com        LOUIS               1   \n",
      "\n",
      "   Shipped Quantity  Unshipped Quantity         Tracking Number  \\\n",
      "0                 2                   0  9400111206210619377583   \n",
      "1                 1                   0  9405511206211488779723   \n",
      "2                 1                   0  9405511206211488779723   \n",
      "3                 1                   0  9405511206211488779723   \n",
      "4                 1                   0  9405511206211488779723   \n",
      "\n",
      "  Sales Order Header Status                 Material Code  \\\n",
      "0                   SHIPPED  AGA-6558RS_BLACK/WHITE_10-12   \n",
      "1                   SHIPPED     AGA-7516RS_BKCSPPROBLUE_M   \n",
      "2                   SHIPPED       AGA-7535RS_BKPROBLWHT_M   \n",
      "3                   SHIPPED       AGX-1354X_BKPROBLWHT_XL   \n",
      "4                   SHIPPED  AGA-7738RS_WHITE/PROMOBLUE_M   \n",
      "\n",
      "                                         Description   Size Shipping Date  \n",
      "0                       RUSH Youth Team Match Shorts  10-12    2024-08-14  \n",
      "1      Adult CS III Printed Interlock Rush SS Jersey      M    2024-09-30  \n",
      "2                          Adult CS III Match Shorts      M    2024-09-30  \n",
      "3  CS ONE Match Soccer Sock with Text Logo  Ankle...     XL    2024-09-30  \n",
      "4  M CS III JSY Adult CS III Printed Interlock Ru...      M    2024-09-30  \n",
      "Aggregated Data Types:\n",
      " Customer Reference                    int64\n",
      "Club Name                            object\n",
      "Date Created                 datetime64[ns]\n",
      "Order Quantity                        int64\n",
      "Shipped Quantity                      int64\n",
      "Unshipped Quantity                    int64\n",
      "Shipping Date                datetime64[ns]\n",
      "Sales Order Header Status            object\n",
      "dtype: object\n",
      "First 5 Rows of Aggregated Data:\n",
      "    Customer Reference             Club Name Date Created  Order Quantity  \\\n",
      "0              567309  Rush National Soccer   2024-01-03               3   \n",
      "1              567586          Rush Coaches   2024-01-06               1   \n",
      "2              567631          Rush Coaches   2024-01-06               2   \n",
      "3              567812              Rush Psd   2024-01-09               1   \n",
      "4              567965              Rush Vsa   2024-01-10               5   \n",
      "\n",
      "   Shipped Quantity  Unshipped Quantity Shipping Date  \\\n",
      "0                 3                   0    2024-01-08   \n",
      "1                 1                   0    2024-01-17   \n",
      "2                 2                   0    2024-01-17   \n",
      "3                 1                   0    2024-01-22   \n",
      "4                 5                   0    2024-02-13   \n",
      "\n",
      "  Sales Order Header Status  \n",
      "0                   SHIPPED  \n",
      "1                   SHIPPED  \n",
      "2                   SHIPPED  \n",
      "3                   SHIPPED  \n",
      "4                   SHIPPED  \n",
      "Aggregated data saved to 'shippingdates/aggregated_orders5.4.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------- Data Loading and Preprocessing -------------------------- #\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the CSV data, preprocesses it by handling missing values, converting data types,\n",
    "    and stripping whitespace from column names and string fields.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the data from the file\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return None\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Could not parse the file '{file_path}'. Please ensure it's a valid CSV.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading the data: {e}\")\n",
    "        return None\n",
    "\n",
    "    df['Shipping Date']=df['Shipped Date']\n",
    "    df = df.drop(\"Shipped Date\", axis = 1)\n",
    "    # Display initial columns and data types\n",
    "    print(\"Initial Columns:\", df.columns.tolist())\n",
    "    print(\"Initial Data Types:\\n\", df.dtypes)\n",
    "    \n",
    "    # Strip whitespace from all column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Replace 'N/A' strings with NaN for proper handling\n",
    "    df.replace('N/A', pd.NA, inplace=True)\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    categorical_cols = [col for col in categorical_cols if col != 'ID']  # Exclude 'ID' if present\n",
    "\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # Convert 'Date Created' and 'Shipping Date' to datetime, coercing errors to NaT\n",
    "    for date_col in ['Date Created', 'Shipping Date']:\n",
    "        if date_col in df.columns:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        else:\n",
    "            print(f\"Warning: '{date_col}' column not found in the data.\")\n",
    "\n",
    "    # Convert quantity columns to numeric, coercing errors to NaN\n",
    "    quantity_cols = ['Order Quantity', 'Shipped Quantity', 'Unshipped Quantity']\n",
    "    for qty_col in quantity_cols:\n",
    "        if qty_col in df.columns:\n",
    "            df[qty_col] = pd.to_numeric(df[qty_col], errors='coerce')\n",
    "        else:\n",
    "            print(f\"Warning: '{qty_col}' column not found in the data.\")\n",
    "\n",
    "    # Handle missing values\n",
    "    # For categorical columns, we will leave NaN as is (do not fill with 'No')\n",
    "    # For numerical columns, fill NaN with the median of each column\n",
    "    df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n",
    "\n",
    "    # Ensure 'ID' is treated as a string to avoid numeric issues\n",
    "    if 'ID' in df.columns:\n",
    "        df['ID'] = df['ID'].astype(str)\n",
    "\n",
    "    # Drop 'Data Type' column if present to avoid serialization issues\n",
    "    if 'Data Type' in df.columns:\n",
    "        df = df.drop('Data Type', axis=1)\n",
    "        print(\"Column 'Data Type' was found and has been excluded from the analysis.\")\n",
    "\n",
    "    # Strip whitespace from specific columns if they contain string values\n",
    "    for col in quantity_cols:\n",
    "        if col in df.columns and df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "    # Display data types after preprocessing\n",
    "    print(\"Data Types After Preprocessing:\\n\", df.dtypes)\n",
    "    print(\"First 5 Rows After Preprocessing:\\n\", df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------- Aggregation Logic -------------------------- #\n",
    "\n",
    "def aggregate_orders(df):\n",
    "    \"\"\"\n",
    "    Aggregates the orders by 'Customer Reference' and 'Club Name'. \n",
    "    Ensures that if any item within a group has a status of 'OPEN', the entire group's status is set to 'OPEN'.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The preprocessed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Aggregated DataFrame.\n",
    "    \"\"\"\n",
    "    def aggregate_status(status_series):\n",
    "        \"\"\"\n",
    "        Aggregates the 'Sales Order Header Status'.\n",
    "        If any status in the group is 'OPEN', return 'OPEN'.\n",
    "        Otherwise, return the most common status or another logic as needed.\n",
    "        \"\"\"\n",
    "        if 'OPEN' in status_series.values:\n",
    "            return 'OPEN'\n",
    "        else:\n",
    "            # If there are multiple statuses and 'OPEN' is not present, define your own logic\n",
    "            # For example, return the mode (most frequent status)\n",
    "            return status_series.mode().iloc[0] if not status_series.mode().empty else 'SHIPPED'\n",
    "\n",
    "    # Perform the aggregation\n",
    "    grouped_df = df.groupby(['Customer Reference', 'Club Name']).agg({\n",
    "        'Date Created': 'min',                        # Earliest Date Created\n",
    "        'Order Quantity': 'sum',                      # Total Order Quantity\n",
    "        'Shipped Quantity': 'sum',                    # Total Shipped Quantity\n",
    "        'Unshipped Quantity': 'sum',                  # Total Unshipped Quantity\n",
    "        'Shipping Date': 'max',                       # Latest Shipping Date\n",
    "        'Sales Order Header Status': aggregate_status  # Custom aggregation for status\n",
    "#         'Home Long Sleeve': 'max',\n",
    "#         'Away Long Sleeve': 'max'\n",
    "        \n",
    "    }).reset_index()\n",
    "\n",
    "    print(\"Aggregated Data Types:\\n\", grouped_df.dtypes)\n",
    "    print(\"First 5 Rows of Aggregated Data:\\n\", grouped_df.head())\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "# -------------------------- Main Execution -------------------------- #\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    input_file = 'shippingdates/Rush Soccer 5.4 - Details.csv'  # Replace with the actual file path\n",
    "    output_file = 'shippingdates/aggregated_orders5.4.csv'                   # Desired output file name\n",
    "\n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data(input_file)\n",
    "    if df is None:\n",
    "        return  # Exit if data loading failed\n",
    "\n",
    "    # Aggregate orders\n",
    "    aggregated_df = aggregate_orders(df)\n",
    "\n",
    "    # Save the aggregated data to a new CSV file\n",
    "    try:\n",
    "        aggregated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Aggregated data saved to '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the aggregated data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4df5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d675b275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Players who purchased long sleeve jerseys:\n",
      "    Player ID              Customer Email Player Name  \\\n",
      "4    623334.0    mvallesteros05@gmail.com        None   \n",
      "11   623343.0             double2@cox.net        None   \n",
      "24   623396.0     Rachel_Donn@hotmail.com        None   \n",
      "25   623397.0    gabrielanano16@yahoo.com        None   \n",
      "32   623410.0  LRisiglione@NevadaRush.com        None   \n",
      "34   623425.0             ibarrjg@msn.com        None   \n",
      "35   623426.0   marybarnett1027@gmail.com        None   \n",
      "36   623427.0          juve5586@gmail.com        None   \n",
      "37   623431.0     jserrano@nevadarush.com        None   \n",
      "43   623486.0     jserrano@nevadarush.com        None   \n",
      "44   623492.0  LRisiglione@NevadaRush.com        None   \n",
      "46   623499.0        roberekson@gmail.com        None   \n",
      "50   623510.0         sgetaneh9@gmail.com        None   \n",
      "52   623522.0         sgetaneh9@gmail.com        None   \n",
      "\n",
      "                     Long Sleeve Status  Home Count  Away Count  \n",
      "4   Purchased Only Away Long Sleeve (1)           0           1  \n",
      "11      Purchased Both Long Sleeves (2)           1           1  \n",
      "24      Purchased Both Long Sleeves (2)           1           1  \n",
      "25  Purchased Only Home Long Sleeve (1)           1           0  \n",
      "32  Purchased Only Away Long Sleeve (1)           0           1  \n",
      "34      Purchased Both Long Sleeves (2)           1           1  \n",
      "35      Purchased Both Long Sleeves (2)           1           1  \n",
      "36      Purchased Both Long Sleeves (2)           1           1  \n",
      "37  Purchased Only Away Long Sleeve (1)           0           1  \n",
      "43  Purchased Only Away Long Sleeve (1)           0           1  \n",
      "44  Purchased Only Away Long Sleeve (1)           0           1  \n",
      "46      Purchased Both Long Sleeves (2)           1           1  \n",
      "50      Purchased Both Long Sleeves (2)           1           1  \n",
      "52      Purchased Both Long Sleeves (2)           1           1  \n",
      "\n",
      "Summary Statistics (Unique Players by Long Sleeve Status):\n",
      "Long Sleeve Status\n",
      "Did Not Purchase Long Sleeve           43\n",
      "Purchased Both Long Sleeves (2)         8\n",
      "Purchased Only Away Long Sleeve (1)     5\n",
      "Purchased Only Home Long Sleeve (1)     1\n",
      "dtype: int64\n",
      "\n",
      "Total Home Long Sleeve Jerseys Purchased: 9\n",
      "Total Away Long Sleeve Jerseys Purchased: 13\n",
      "\n",
      "Files saved: '4.13Order_Summary_with_ls_status.csv' and 'Rush Soccer 4.13 - Details_with_ls_status.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame.\n",
    "df = pd.read_csv(\"shippingdates/Rush Soccer 4.27 - Rush Details Category 2025.csv\")\n",
    "\n",
    "# OPTIONAL: To restrict analysis to a specific club, e.g., \"Rush Montana\", uncomment:\n",
    "df = df[df[\"Club Name\"] == \"Rush Nevada\"]\n",
    "df = df[df[\"Category\"].isin([\"Field Players Mandatory Kit\", \"Goalkeepers Mandatory Kit\",\"Competitive Items\"])]\n",
    "\n",
    "# Convert \"Order Date\" to datetime and filter for orders in 2025.\n",
    "df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"], errors=\"coerce\")\n",
    "df = df[df[\"Order Date\"].dt.year == 2025]\n",
    "\n",
    "# Define the long sleeve product descriptions.\n",
    "home_ls = \"NEVADA RUSH BROOKLYN II RUSH SOCCER PYRAMIDS LONG SLEEVE MATCH JERSEY PROMO BLUE BLACK WHITE\"\n",
    "away_ls = \"NEVADA RUSH BROOKLYN II RUSH SOCCER SCATTERED SHARDE LONG SLEEVE MATCH JERSEY WHITE PROMO GREY BLACK\"\n",
    "\n",
    "# Function to compute long sleeve status and counts from a list of product names.\n",
    "def compute_ls_status_player(product_names):\n",
    "    # Convert the list into a pandas Series for easier aggregation.\n",
    "    s = pd.Series(product_names)\n",
    "    count_home = s.isin([home_ls]).sum()\n",
    "    count_away = s.isin([away_ls]).sum()\n",
    "    total = count_home + count_away\n",
    "    if total == 0:\n",
    "        status = \"Did Not Purchase Long Sleeve\"\n",
    "    elif count_home > 0 and count_away > 0:\n",
    "        status = f\"Purchased Both Long Sleeves ({total})\"\n",
    "    elif count_home > 0:\n",
    "        status = f\"Purchased Only Home Long Sleeve ({count_home})\"\n",
    "    elif count_away > 0:\n",
    "        status = f\"Purchased Only Away Long Sleeve ({count_away})\"\n",
    "    else:\n",
    "        status = \"Did Not Purchase Long Sleeve\"\n",
    "    return status, count_home, count_away\n",
    "\n",
    "# Group by \"Player ID\" so that if a player purchases in multiple orders, they are aggregated together.\n",
    "grouped = df.groupby(\"Player ID\").agg({\n",
    "    \"Order ID\": lambda x: \" ; \".join(x.astype(str).unique()),\n",
    "    \"Order Date\": \"min\",             # earliest order date for that player\n",
    "    \"Customer Email\": \"first\",       # assuming one email per player\n",
    "    \"Club Name\": \"first\",\n",
    "    \"Player Name\": \"first\",\n",
    "    \"Product Name\": lambda x: list(x)  # aggregate all product names into a list\n",
    "}).reset_index()\n",
    "\n",
    "# Compute long sleeve status and counts for each player.\n",
    "grouped[[\"Long Sleeve Status\", \"Home Count\", \"Away Count\"]] = grouped[\"Product Name\"].apply(\n",
    "    lambda prod_list: pd.Series(compute_ls_status_player(prod_list))\n",
    ")\n",
    "\n",
    "# Print out unique players (Player ID, Customer Email, Player Name, and status) for players who purchased any long sleeve.\n",
    "purchased_ls = grouped[grouped[\"Long Sleeve Status\"] != \"Did Not Purchase Long Sleeve\"]\n",
    "print(\"Players who purchased long sleeve jerseys:\")\n",
    "print(purchased_ls[[\"Player ID\", \"Customer Email\", \"Player Name\", \"Long Sleeve Status\", \"Home Count\", \"Away Count\"]])\n",
    "\n",
    "# Print summary statistics: count of unique players by Long Sleeve Status.\n",
    "summary = grouped.groupby(\"Long Sleeve Status\").size()\n",
    "print(\"\\nSummary Statistics (Unique Players by Long Sleeve Status):\")\n",
    "print(summary)\n",
    "\n",
    "# Print total counts of Home and Away long sleeve jerseys purchased across all players.\n",
    "total_home = grouped[\"Home Count\"].sum()\n",
    "total_away = grouped[\"Away Count\"].sum()\n",
    "print(f\"\\nTotal Home Long Sleeve Jerseys Purchased: {total_home}\")\n",
    "print(f\"Total Away Long Sleeve Jerseys Purchased: {total_away}\")\n",
    "\n",
    "# Write the aggregated order summary to CSV in the specified directory.\n",
    "grouped.to_csv(\"LongSleeveOrders/4.13Order_Summary_with_ls_status.csv\", index=False)\n",
    "\n",
    "# Optionally, merge the summary information back into the original DataFrame by Player ID.\n",
    "df = df.merge(grouped[[\"Player ID\", \"Long Sleeve Status\", \"Home Count\", \"Away Count\"]],\n",
    "              on=\"Player ID\", how=\"left\")\n",
    "df.to_csv(\"LongSleeveOrders/Rush Soccer 4.13 - Details_with_ls_status.csv\", index=False)\n",
    "\n",
    "print(\"\\nFiles saved: '4.13Order_Summary_with_ls_status.csv' and 'Rush Soccer 4.13 - Details_with_ls_status.csv'\")\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV file into a DataFrame. Adjust the filename as needed.\n",
    "# df = pd.read_csv(\"shippingdates/Rush Soccer 3.2 NEW - Rush Details Category 2025.csv\")\n",
    "\n",
    "# # OPTIONAL: To restrict analysis to a specific club (e.g., \"Rush Montana\"), uncomment:\n",
    "# df = df[df[\"Club Name\"] == \"Rush Montana\"]\n",
    "\n",
    "# # Convert \"Order Date\" to datetime and filter for orders placed in 2025.\n",
    "# df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"], errors=\"coerce\")\n",
    "# df = df[df[\"Order Date\"].dt.year == 2025]\n",
    "\n",
    "# # Define the long sleeve product descriptions.\n",
    "# home_ls = \"MONTANA RUSH BROOKLYN II RUSH SOCCER PYRAMIDS LONG SLEEVE MATCH JERSEY PROMO BLUE BLACK WHITE\"\n",
    "# away_ls = \"MONTANA RUSH BROOKLYN II RUSH SOCCER SCATTERED SHARDE LONG SLEEVE MATCH JERSEY WHITE PROMO GREY BLACK\"\n",
    "\n",
    "# # Function to compute long sleeve status and counts from a list of product names.\n",
    "# def compute_ls_status(descriptions):\n",
    "#     # Convert the list to a pandas Series for easier calculation.\n",
    "#     desc_series = pd.Series(descriptions)\n",
    "#     count_home = desc_series.isin([home_ls]).sum()\n",
    "#     count_away = desc_series.isin([away_ls]).sum()\n",
    "#     total = count_home + count_away\n",
    "#     if total == 0:\n",
    "#         status = \"Did Not Purchase Long Sleeve\"\n",
    "#     elif count_home > 0 and count_away > 0:\n",
    "#         status = f\"Purchased Both Long Sleeves ({total})\"\n",
    "#     elif count_home > 0:\n",
    "#         status = f\"Purchased Only Home Long Sleeve ({count_home})\"\n",
    "#     elif count_away > 0:\n",
    "#         status = f\"Purchased Only Away Long Sleeve ({count_away})\"\n",
    "#     else:\n",
    "#         status = \"Did Not Purchase Long Sleeve\"\n",
    "#     return status, count_home, count_away\n",
    "\n",
    "# # Group by \"Order ID\" and \"Player ID\" so that we aggregate items per player in an order.\n",
    "# order_summary = df.groupby([\"Customer Reference\", \"Player ID\"]).agg({\n",
    "#     \"Order Date\": \"first\",           # Take the first order date per group.\n",
    "#     \"Customer Email\": \"first\",       # Assume each order/player has one email.\n",
    "#     \"Club Name\": \"first\",\n",
    "#     \"Player Name\": \"first\",\n",
    "#     \"Product Name\": lambda x: list(x)  # Aggregate all product names for this order/player.\n",
    "# }).reset_index()\n",
    "\n",
    "# # Compute long sleeve status and counts for each group.\n",
    "# order_summary[[\"Long Sleeve Status\", \"Home Count\", \"Away Count\"]] = order_summary[\"Product Name\"].apply(\n",
    "#     lambda desc: pd.Series(compute_ls_status(desc))\n",
    "# )\n",
    "\n",
    "# # Print unique orders (Order ID, Player ID, Email, and status) that purchased any long sleeve.\n",
    "# purchased_ls = order_summary[order_summary[\"Long Sleeve Status\"] != \"Did Not Purchase Long Sleeve\"]\n",
    "# print(\"Orders that purchased any long sleeve:\")\n",
    "# print(purchased_ls[[\"Customer Reference\", \"Player ID\", \"Customer Email\", \"Player Name\", \"Long Sleeve Status\", \"Home Count\", \"Away Count\"]])\n",
    "\n",
    "# # Print summary statistics: count of unique orders by Long Sleeve Status.\n",
    "# summary = order_summary.groupby(\"Long Sleeve Status\").size()\n",
    "# print(\"\\nSummary Statistics (Unique Orders by Long Sleeve Status):\")\n",
    "# print(summary)\n",
    "\n",
    "# # Print total counts of Home and Away long sleeve jerseys across all orders.\n",
    "# total_home = order_summary[\"Home Count\"].sum()\n",
    "# total_away = order_summary[\"Away Count\"].sum()\n",
    "# print(f\"\\nTotal Home Long Sleeve Jerseys Purchased: {total_home}\")\n",
    "# print(f\"Total Away Long Sleeve Jerseys Purchased: {total_away}\")\n",
    "\n",
    "# # Write the order summary DataFrame to CSV in the designated directory.\n",
    "# order_summary.to_csv(\"LongSleeveOrders/3.2Order_Summary_with_ls_status.csv\", index=False)\n",
    "\n",
    "# # Optionally, merge the computed status back into the original DataFrame for a full export.\n",
    "# df = df.merge(order_summary[[\"Customer Reference\", \"Player ID\", \"Long Sleeve Status\", \"Home Count\", \"Away Count\"]],\n",
    "#               on=[\"Customer Reference\", \"Player ID\"], how=\"left\")\n",
    "# df.to_csv(\"LongSleeveOrders/Rush Soccer 3.2 - Details_with_ls_status.csv\", index=False)\n",
    "\n",
    "# print(\"\\nFiles saved: '3.2Order_Summary_with_ls_status.csv' and 'Rush Soccer 3.2 - Details_with_ls_status.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5d948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958b8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "617107\n",
    "# import pandas as pd\n",
    "\n",
    "# # ------------------------- 1. Load the Data ------------------------- #\n",
    "\n",
    "# # Load the data from the file\n",
    "# file_path = 'shippingdates/Rush Soccer 12.15 - Details.csv'  # Replace with the actual file path\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Display column names and data types\n",
    "# print(\"Column Names:\", df.columns.tolist())\n",
    "# print(\"\\nData Types Before Cleaning:\\n\", df.dtypes)\n",
    "\n",
    "# # ------------------------- 2. Clean Column Names ------------------------- #\n",
    "\n",
    "# # Strip whitespace from all column names\n",
    "# df.columns = df.columns.str.strip()\n",
    "\n",
    "# # If needed, rename columns to remove internal spaces or correct names\n",
    "# # Example:\n",
    "# # df.rename(columns={\" Order Quantity \": \"Order Quantity\", \n",
    "# #                    \" Shipped Quantity \": \"Shipped Quantity\", \n",
    "# #                    ' Unshipped Quantity ': 'Unshipped Quantity'}, inplace=True)\n",
    "\n",
    "# # ------------------------- 3. Convert Quantity Columns to Numeric ------------------------- #\n",
    "\n",
    "# # Define the quantity columns\n",
    "# quantity_columns = ['Order Quantity', 'Shipped Quantity', 'Unshipped Quantity']\n",
    "\n",
    "# # Check if these columns exist\n",
    "# missing_columns = [col for col in quantity_columns if col not in df.columns]\n",
    "# if missing_columns:\n",
    "#     print(f\"Warning: The following quantity columns are missing: {missing_columns}\")\n",
    "#     # Optionally, handle missing columns here (e.g., create them with default values)\n",
    "# else:\n",
    "#     # Convert to numeric, coercing errors to NaN\n",
    "#     for col in quantity_columns:\n",
    "#         df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "#     # Replace NaN values with 0\n",
    "#     df[quantity_columns] = df[quantity_columns].fillna(0)\n",
    "\n",
    "#     # Verify data types after conversion\n",
    "#     print(\"\\nData Types After Conversion:\\n\", df.dtypes)\n",
    "\n",
    "#     # Display the first few rows after cleaning\n",
    "#     print(\"\\nData Sample After Cleaning:\")\n",
    "#     print(df.head())\n",
    "\n",
    "#     # ------------------------- 4. Perform Aggregation ------------------------- #\n",
    "\n",
    "#     # Group by 'Customer Reference' and 'Club Name' and aggregate\n",
    "#     grouped_df = df.groupby(['Customer Reference', 'Club Name']).agg({\n",
    "#         'Date Created': 'min',                        # Minimum of Date Created\n",
    "#         'Order Quantity': 'sum',                      # Sum of Order Quantity\n",
    "#         'Shipped Quantity': 'sum',                    # Sum of Shipped Quantity\n",
    "#         'Unshipped Quantity': 'sum',                  # Sum of Unshipped Quantity\n",
    "#         'Shipping Date': 'max',                       # Maximum of Shipping Date\n",
    "#         'Sales Order Header Status': 'last'           # Take the last status\n",
    "#     }).reset_index()\n",
    "\n",
    "#     # Display the aggregated data\n",
    "#     print(\"\\nAggregated Data Sample:\")\n",
    "#     print(grouped_df.head())\n",
    "\n",
    "#     # ------------------------- 5. Save Aggregated Data ------------------------- #\n",
    "\n",
    "#     # Save the aggregated data to a new CSV file\n",
    "#     output_file = 'aggregated_orders12.15.csv'  # Replace with desired output file name\n",
    "#     grouped_df.to_csv(output_file, index=False)\n",
    "\n",
    "#     print(f\"\\nAggregated data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "490ef52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # packages_shipped_analysis.ipynb\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # -------------------------- Step 1: Load the Data -------------------------- #\n",
    "\n",
    "# # Define the file path\n",
    "# file_path = 'shippingdates/Rush Soccer 11.24.24 - Details.csv'  # Replace with your actual file path\n",
    "\n",
    "# # Load the CSV file into a pandas DataFrame\n",
    "# # Assuming the CSV is tab-separated based on the sample data\n",
    "# try:\n",
    "#     df = pd.read_csv(file_path, sep='\\t', engine='python')\n",
    "#     print(\"Data loaded successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading data: {e}\")\n",
    "\n",
    "# # -------------------------- Step 2: Inspect the Data -------------------------- #\n",
    "\n",
    "# # Display the first few rows to understand the data structure\n",
    "# print(\"First 5 rows of the dataset:\")\n",
    "# print(df.head())\n",
    "\n",
    "# # Display the column names and their data types\n",
    "# print(\"\\nColumn Names:\")\n",
    "# print(df.columns)\n",
    "# print(\"\\nData Types:\")\n",
    "# print(df.dtypes)\n",
    "\n",
    "# # -------------------------- Step 3: Clean Column Names -------------------------- #\n",
    "\n",
    "# # Strip leading and trailing whitespace from all column names\n",
    "# df.columns = df.columns.str.strip()\n",
    "\n",
    "# # Verify column names after stripping\n",
    "# print(\"\\nCleaned Column Names:\")\n",
    "# print(df.columns)\n",
    "\n",
    "# # -------------------------- Step 4: Clean Specific Columns -------------------------- #\n",
    "\n",
    "# # Define columns that may contain whitespace and need to be stripped\n",
    "# columns_to_strip = ['Order Quantity', 'Shipped Quantity', 'Unshipped Quantity']\n",
    "\n",
    "# # Strip whitespace from these columns if they are of object type (strings)\n",
    "# df[columns_to_strip] = df[columns_to_strip].apply(\n",
    "#     lambda x: x.str.strip() if x.dtype == \"object\" else x\n",
    "# )\n",
    "\n",
    "# # Convert 'Shipped Quantity' and 'Unshipped Quantity' to numeric types\n",
    "# # Replace non-numeric entries with 0\n",
    "# df['Shipped Quantity'] = pd.to_numeric(df['Shipped Quantity'], errors='coerce').fillna(0).astype(int)\n",
    "# df['Unshipped Quantity'] = pd.to_numeric(df['Unshipped Quantity'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# # Verify the changes\n",
    "# print(\"\\nData Types After Conversion:\")\n",
    "# print(df.dtypes)\n",
    "\n",
    "# # -------------------------- Step 5: Convert Date Columns -------------------------- #\n",
    "\n",
    "# # Convert 'Date Created' and 'Shipping Date' to datetime format\n",
    "# # Coerce errors to NaT (Not a Time) for invalid dates\n",
    "# df['Date Created'] = pd.to_datetime(df['Date Created'], errors='coerce', format='%m/%d/%Y')\n",
    "# df['Shipping Date'] = pd.to_datetime(df['Shipping Date'], errors='coerce', format='%m/%d/%Y')\n",
    "\n",
    "# # Verify the conversion\n",
    "# print(\"\\nDate Columns After Conversion:\")\n",
    "# print(df[['Date Created', 'Shipping Date']].head())\n",
    "\n",
    "# # -------------------------- Step 6: Handle Invalid 'Shipping Date' Entries -------------------------- #\n",
    "\n",
    "# # Identify rows with invalid 'Shipping Date' (NaT)\n",
    "# invalid_shipping_dates = df['Shipping Date'].isna()\n",
    "# print(f\"\\nNumber of rows with invalid 'Shipping Date': {invalid_shipping_dates.sum()}\")\n",
    "\n",
    "# # Option 1: Remove rows with invalid 'Shipping Date'\n",
    "# df_clean = df.dropna(subset=['Shipping Date']).copy()\n",
    "# print(f\"Number of rows after removing invalid 'Shipping Date': {df_clean.shape[0]}\")\n",
    "\n",
    "# # Optionally, you can choose to fill invalid 'Shipping Date' with 'Date Created' or another default date\n",
    "# # Uncomment the following lines if you prefer this approach\n",
    "# # df['Shipping Date'] = df['Shipping Date'].fillna(df['Date Created'])\n",
    "# # df_clean = df.dropna(subset=['Shipping Date']).copy()\n",
    "\n",
    "# # -------------------------- Step 7: Extract Month-Year from 'Shipping Date' -------------------------- #\n",
    "\n",
    "# # Create a new column 'Month-Year' in 'MMMM YYYY' format (e.g., July 2024)\n",
    "# df_clean['Month-Year'] = df_clean['Shipping Date'].dt.strftime('%B %Y')\n",
    "\n",
    "# # Verify the new column\n",
    "# print(\"\\nSample 'Month-Year' Entries:\")\n",
    "# print(df_clean[['Shipping Date', 'Month-Year']].head())\n",
    "\n",
    "# # -------------------------- Step 8: Remove Duplicate Tracking Numbers -------------------------- #\n",
    "\n",
    "# # Assuming each 'Tracking Number' uniquely identifies a package, remove duplicates\n",
    "# # If 'Tracking Number' is not unique per package, adjust accordingly\n",
    "# df_unique = df_clean.drop_duplicates(subset=['Tracking Number'])\n",
    "\n",
    "# # Verify the removal of duplicates\n",
    "# print(f\"\\nNumber of unique packages after removing duplicates: {df_unique.shape[0]}\")\n",
    "\n",
    "# # -------------------------- Step 9: Group by 'Month-Year' and Count Unique Packages -------------------------- #\n",
    "\n",
    "# # Group by 'Month-Year' and count unique 'Tracking Number' to get the number of packages shipped each month\n",
    "# packages_shipped = df_unique.groupby('Month-Year')['Tracking Number'].nunique().reset_index()\n",
    "\n",
    "# # Rename the columns for clarity\n",
    "# packages_shipped.columns = ['Month-Year', 'Unique Packages Shipped']\n",
    "\n",
    "# # -------------------------- Step 10: Sort the Results Chronologically -------------------------- #\n",
    "\n",
    "# # Convert 'Month-Year' back to datetime for sorting\n",
    "# packages_shipped['Month-Year-Date'] = pd.to_datetime(packages_shipped['Month-Year'], format='%B %Y')\n",
    "\n",
    "# # Sort by the new datetime column\n",
    "# packages_shipped = packages_shipped.sort_values('Month-Year-Date')\n",
    "\n",
    "# # Drop the auxiliary datetime column\n",
    "# packages_shipped = packages_shipped.drop('Month-Year-Date', axis=1)\n",
    "\n",
    "# # -------------------------- Step 11: Display the Results -------------------------- #\n",
    "\n",
    "# print(\"\\nNumber of Packages Shipped Each Month:\")\n",
    "# print(packages_shipped)\n",
    "\n",
    "# # -------------------------- Step 12: (Optional) Save the Results to CSV -------------------------- #\n",
    "\n",
    "# # Define the output file path\n",
    "# output_file = 'packages_shipped_per_month.csv'\n",
    "\n",
    "# # Save the DataFrame to a new CSV file\n",
    "# packages_shipped.to_csv(output_file, index=False)\n",
    "# print(f\"\\nAggregated data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd848b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39340ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at shippingdates/Capelli_Historical_2023_Data_Backup.csv\n",
      "Columns in the DataFrame:\n",
      "['Order ID', 'Order Date', 'Ship Date', 'Customer Email', 'Customer Name', 'Club Name', 'Product Sku', 'Material', 'Product Name', 'Product Quantity']\n",
      "\n",
      "Data Types:\n",
      "Order ID             int64\n",
      "Order Date          object\n",
      "Ship Date           object\n",
      "Customer Email      object\n",
      "Customer Name       object\n",
      "Club Name           object\n",
      "Product Sku         object\n",
      "Material            object\n",
      "Product Name        object\n",
      "Product Quantity     int64\n",
      "dtype: object\n",
      "\n",
      "First 5 Rows:\n",
      "   Order ID  Order Date Ship Date             Customer Email  \\\n",
      "0   1129547  12/13/2023  1/2/2024  mychelle_gaynor@yahoo.com   \n",
      "1   1129547  12/13/2023  1/2/2024  mychelle_gaynor@yahoo.com   \n",
      "2   1129547  12/13/2023  1/2/2024  mychelle_gaynor@yahoo.com   \n",
      "3   1131083  12/26/2023  1/2/2024      heidir392@hotmail.com   \n",
      "4   1131083  12/26/2023  1/2/2024      heidir392@hotmail.com   \n",
      "\n",
      "       Customer Name       Club Name                      Product Sku  \\\n",
      "0  Mychelle  Mcglynn   Rush Missouri    AGA-2064X-2_BLKWHT_AL_3RUMOFS   \n",
      "1  Mychelle  Mcglynn   Rush Missouri     AGA-2154-1_BLKWHT_AL_3RUMOFS   \n",
      "2  Mychelle  Mcglynn   Rush Missouri       AGA-7240_BLKWHT_AS_3RUMOFS   \n",
      "3         Heidi Reed  Rush Wisconsin  AGX-5140RS_BLKPRBLWHT_O_3RUWIBM   \n",
      "4         Heidi Reed  Rush Wisconsin      AGX-1086X_PRBLWHT_L_3RUWIBM   \n",
      "\n",
      "                      Material  \\\n",
      "0    AGA-2064X-2_BLACK/WHITE_L   \n",
      "1     AGA-2154-1_BLACK/WHITE_L   \n",
      "2       AGA-7240_BLACK/WHITE_S   \n",
      "3     AGX-5140RS_BKPROBLWHT_OS   \n",
      "4  AGX-1086X_PROMOBLUE/WHITE_L   \n",
      "\n",
      "                                        Product Name  Product Quantity  \n",
      "0  MISSOURI RUSH   BASICS TEE SHIRT LOGO CENTER F...                 1  \n",
      "1  MISSOURI RUSH  BASIC LONG SLEEVE TEE SHIRT LOG...                 1  \n",
      "2  MISSOURI RUSH   BASICS FLEECE SWEATPANTS BLACK...                 1  \n",
      "3  WISCONSIN RUSH PITCH I/ BACKPACK BLACK PROMO B...                 1  \n",
      "4  WISCONSIN RUSH CS ONE SOFT I SOCKS PROMO BLUE ...                 1  \n",
      "\n",
      "Standardized Columns:\n",
      "['order_id', 'order_date', 'ship_date', 'customer_email', 'customer_name', 'club_name', 'product_sku', 'material', 'product_name', 'product_quantity']\n",
      "\n",
      "DataFrame after selecting required columns:\n",
      "   order_id  order_date ship_date  product_quantity       club_name\n",
      "0   1129547  12/13/2023  1/2/2024                 1   Rush Missouri\n",
      "1   1129547  12/13/2023  1/2/2024                 1   Rush Missouri\n",
      "2   1129547  12/13/2023  1/2/2024                 1   Rush Missouri\n",
      "3   1131083  12/26/2023  1/2/2024                 1  Rush Wisconsin\n",
      "4   1131083  12/26/2023  1/2/2024                 1  Rush Wisconsin\n",
      "\n",
      "First 5 Rows after Cleaning:\n",
      "   order_id order_date  ship_date  product_quantity       club_name\n",
      "0   1129547 2023-12-13 2024-01-02                 1   Rush Missouri\n",
      "1   1129547 2023-12-13 2024-01-02                 1   Rush Missouri\n",
      "2   1129547 2023-12-13 2024-01-02                 1   Rush Missouri\n",
      "3   1131083 2023-12-26 2024-01-02                 1  Rush Wisconsin\n",
      "4   1131083 2023-12-26 2024-01-02                 1  Rush Wisconsin\n",
      "\n",
      "Missing Values in Key Columns:\n",
      "order_id            0\n",
      "order_date          0\n",
      "ship_date           0\n",
      "product_quantity    0\n",
      "club_name           0\n",
      "dtype: int64\n",
      "\n",
      "Aggregated Data:\n",
      "   order_id         Club Name First Order Date Latest Ship Date  \\\n",
      "0    468931  Rush Ny Syracuse       2023-01-01       2023-01-10   \n",
      "1    468940          Rush Lfa       2023-01-01       2023-02-09   \n",
      "2    468965    Rush Flatirons       2023-01-01       2023-01-12   \n",
      "3    468966  Rush Atletico Nv       2023-01-01       2023-02-14   \n",
      "4    468971    Rush United Fc       2023-01-01       2023-01-12   \n",
      "\n",
      "   Total Products Ordered  \n",
      "0                       2  \n",
      "1                       8  \n",
      "2                       4  \n",
      "3                       4  \n",
      "4                       6  \n",
      "\n",
      "Aggregated data saved to Capelli_aggregated_orders.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# ================================\n",
    "# 1. Backup the Original CSV File (Optional)\n",
    "# ================================\n",
    "\n",
    "original_file = 'shippingdates/Capelli Historical 2023 Data - Details.csv'  # Original data file\n",
    "aggregated_file = 'Capelli_aggregated_orders.csv'  # Aggregated data file\n",
    "\n",
    "backup_file = 'shippingdates/Capelli_Historical_2023_Data_Backup.csv'\n",
    "shutil.copyfile(original_file, backup_file)\n",
    "print(f\"Backup created at {backup_file}\")\n",
    "\n",
    "# ================================\n",
    "# 2. Load and Inspect the Data\n",
    "# ================================\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(original_file)\n",
    "\n",
    "# Display initial columns and data types\n",
    "print(\"Columns in the DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# ================================\n",
    "# 3. Clean and Standardize Column Names\n",
    "# ================================\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_names(columns):\n",
    "    columns = columns.str.lower().str.strip()  # Convert to lowercase and strip whitespace\n",
    "    columns = columns.str.replace('[?/(),]', '', regex=True)  # Remove special characters\n",
    "    columns = columns.str.replace(' ', '_')  # Replace spaces with underscores\n",
    "    return columns\n",
    "\n",
    "# Apply the function to clean column names\n",
    "df.columns = clean_column_names(df.columns)\n",
    "print(\"\\nStandardized Columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# ================================\n",
    "# 4. Select Only Required Columns\n",
    "# ================================\n",
    "\n",
    "# Define the required columns\n",
    "required_columns = ['order_id', 'order_date', 'ship_date', 'product_quantity', 'club_name']\n",
    "\n",
    "# Check if all required columns exist\n",
    "missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Error: The following required columns are missing from the DataFrame: {missing_cols}\")\n",
    "else:\n",
    "    # Select only the required columns\n",
    "    df = df[required_columns]\n",
    "    print(\"\\nDataFrame after selecting required columns:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # ================================\n",
    "    # 5. Clean Specific Columns\n",
    "    # ================================\n",
    "\n",
    "    # Strip whitespace from 'order_id' if it's a string\n",
    "    if df['order_id'].dtype == object:\n",
    "        df['order_id'] = df['order_id'].str.strip()\n",
    "\n",
    "    # Convert 'order_date' and 'ship_date' to datetime format\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')\n",
    "    df['ship_date'] = pd.to_datetime(df['ship_date'], errors='coerce')\n",
    "\n",
    "    # Convert 'product_quantity' to numeric and handle missing values\n",
    "    df['product_quantity'] = pd.to_numeric(df['product_quantity'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Display first 5 rows after cleaning\n",
    "    print(\"\\nFirst 5 Rows after Cleaning:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Check for missing values in key columns\n",
    "    print(\"\\nMissing Values in Key Columns:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Handle missing values if necessary\n",
    "    # For example, drop rows where 'order_id' is missing\n",
    "    df = df.dropna(subset=['order_id'])\n",
    "\n",
    "    # ================================\n",
    "    # 6. Aggregate the Data\n",
    "    # ================================\n",
    "\n",
    "    # Group by 'order_id' and aggregate\n",
    "    grouped_df = df.groupby('order_id').agg({\n",
    "        'club_name': 'first',              # Assuming one club per order\n",
    "        'order_date': 'min',               # Earliest order date\n",
    "        'ship_date': 'max',                # Latest ship date\n",
    "        'product_quantity': 'sum'          # Total products ordered\n",
    "    }).reset_index()\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    grouped_df.rename(columns={\n",
    "        'club_name': 'Club Name',\n",
    "        'order_date': 'First Order Date',\n",
    "        'ship_date': 'Latest Ship Date',\n",
    "        'product_quantity': 'Total Products Ordered'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Display the aggregated data\n",
    "    print(\"\\nAggregated Data:\")\n",
    "    print(grouped_df.head())\n",
    "\n",
    "    # ================================\n",
    "    # 7. Save the Aggregated Data Back to the Same CSV File\n",
    "    # ================================\n",
    "\n",
    "    # Save the aggregated data to the same CSV file\n",
    "    grouped_df.to_csv(aggregated_file, index=False)\n",
    "    print(f\"\\nAggregated data saved to {aggregated_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f5892f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import shutil\n",
    "\n",
    "# # ================================\n",
    "# # 1. Backup the Original CSV File\n",
    "# # ================================\n",
    "\n",
    "# input_file = 'Capelli_aggregated_orders.csv'\n",
    "# backup_file = 'Capelli_aggregated_orders_backup.csv'\n",
    "\n",
    "# # Create a backup\n",
    "# shutil.copyfile(input_file, backup_file)\n",
    "# print(f\"Backup created at {backup_file}\")\n",
    "\n",
    "# # ================================\n",
    "# # 2. Load and Inspect the Data\n",
    "# # ================================\n",
    "\n",
    "# # Load the data\n",
    "# df = pd.read_csv(input_file)\n",
    "\n",
    "# # Display the first few rows and column information\n",
    "# print(\"Initial Columns:\")\n",
    "# print(df.columns.tolist())\n",
    "# print(\"\\nData Types:\")\n",
    "# print(df.dtypes)\n",
    "# print(\"\\nFirst 5 Rows:\")\n",
    "# print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
