{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5c854b",
   "metadata": {},
   "source": [
    "## Order Grouping & Aggregation Notebook\n",
    "\n",
    "This notebook ingests the weekly _“Details”_ CSV export from Capelli, cleans and normalizes the data, and then produces an aggregated orders report that is written back into the `shippingdates/` directory.  \n",
    "\n",
    "---\n",
    "\n",
    "### ⭐ What This Notebook Does  \n",
    "1. **Load & Preprocess**  \n",
    "   - Reads in the Capelli _Details_ CSV using `load_and_preprocess_data(file_path)`  \n",
    "   - Renames and converts the raw “Shipped Date” to a uniform `Shipping Date` column  \n",
    "   - Strips whitespace, coerces date and numeric types, replaces `'N/A'` with `NaN`, and fills missing numeric values with medians  \n",
    "2. **Aggregate Orders**  \n",
    "   - Groups by **Customer Reference** and **Club Name**  \n",
    "   - Sums quantities (`Order Quantity`, `Shipped Quantity`, `Unshipped Quantity`), picks earliest `Date Created`, latest `Shipping Date`  \n",
    "   - Custom-aggregates order status so that any subgroup with an “OPEN” status yields “OPEN” for the whole group  \n",
    "3. **Save Output**  \n",
    "   - Writes the aggregated DataFrame to a new CSV in `shippingdates/` (e.g. `aggregated_orders5.4.csv`)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Weekly Updates Required  \n",
    "- **`input_file`**:  \n",
    "  - Edit this path (inside the `main()` cell) to point to your newly downloaded “Details” CSV (e.g.  \n",
    "    ```python\n",
    "    input_file = 'shippingdates/Rush Soccer 5.11 - Details.csv'\n",
    "    ```  \n",
    "- **`output_file`**:  \n",
    "  - Change the filename to match the week’s aggregated report (e.g.  \n",
    "    ```python\n",
    "    output_file = 'shippingdates/aggregated_orders5.11.csv'\n",
    "    ```  \n",
    "- **Re-run** all cells from top to bottom to generate and persist the new output file.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 How to Run  \n",
    "1. Upload the latest Capelli “Details” CSV into `shippingdates/`  \n",
    "2. Update `input_file` and `output_file` in the final code block  \n",
    "3. Execute **all** cells (`Cell ▶ Run All`)  \n",
    "4. Verify that the new `aggregated_orders*.csv` appears in `shippingdates/`\n",
    "\n",
    "> **Tip**: Commit only the new `aggregated_orders*.csv` file each week; do _not_ commit interim or backup spreadsheets.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*References:*  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c81b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Initial Columns: ['Customer Reference', 'Club Name', 'Date Created', 'Sold TO Name', 'Sold TO Email', 'Ship TO Name', 'Order Quantity', 'Shipped Quantity', 'Unshipped Quantity', 'Tracking Number', 'Sales Order Header Status', 'Material Code', 'Description', 'Size', 'Shipping Date']\n",
      "Initial Data Types:\n",
      " Customer Reference            int64\n",
      "Club Name                    object\n",
      "Date Created                 object\n",
      "Sold TO Name                 object\n",
      "Sold TO Email                object\n",
      "Ship TO Name                 object\n",
      "Order Quantity                int64\n",
      "Shipped Quantity              int64\n",
      "Unshipped Quantity            int64\n",
      "Tracking Number              object\n",
      "Sales Order Header Status    object\n",
      "Material Code                object\n",
      "Description                  object\n",
      "Size                         object\n",
      "Shipping Date                object\n",
      "dtype: object\n",
      "Data Types After Preprocessing:\n",
      " Customer Reference                    int64\n",
      "Club Name                            object\n",
      "Date Created                 datetime64[ns]\n",
      "Sold TO Name                         object\n",
      "Sold TO Email                        object\n",
      "Ship TO Name                         object\n",
      "Order Quantity                        int64\n",
      "Shipped Quantity                      int64\n",
      "Unshipped Quantity                    int64\n",
      "Tracking Number                      object\n",
      "Sales Order Header Status            object\n",
      "Material Code                        object\n",
      "Description                          object\n",
      "Size                                 object\n",
      "Shipping Date                datetime64[ns]\n",
      "dtype: object\n",
      "Aggregated Data Types:\n",
      " Customer Reference                    int64\n",
      "Club Name                            object\n",
      "Date Created                 datetime64[ns]\n",
      "Order Quantity                        int64\n",
      "Shipped Quantity                      int64\n",
      "Unshipped Quantity                    int64\n",
      "Shipping Date                datetime64[ns]\n",
      "Sales Order Header Status            object\n",
      "dtype: object\n",
      "First 5 Rows of Aggregated Data:\n",
      "    Customer Reference             Club Name Date Created  Order Quantity  \\\n",
      "0              567309  Rush National Soccer   2024-01-03               3   \n",
      "1              567586          Rush Coaches   2024-01-06               1   \n",
      "2              567631          Rush Coaches   2024-01-06               2   \n",
      "3              567812              Rush Psd   2024-01-09               1   \n",
      "4              567965              Rush Vsa   2024-01-10               5   \n",
      "\n",
      "   Shipped Quantity  Unshipped Quantity Shipping Date  \\\n",
      "0                 3                   0    2024-01-08   \n",
      "1                 1                   0    2024-01-17   \n",
      "2                 2                   0    2024-01-17   \n",
      "3                 1                   0    2024-01-22   \n",
      "4                 5                   0    2024-02-13   \n",
      "\n",
      "  Sales Order Header Status  \n",
      "0                   SHIPPED  \n",
      "1                   SHIPPED  \n",
      "2                   SHIPPED  \n",
      "3                   SHIPPED  \n",
      "4                   SHIPPED  \n",
      "Aggregated data saved to 'shippingdates/aggregated_orders5.4.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------- Data Loading and Preprocessing -------------------------- #\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the CSV data, preprocesses it by handling missing values, converting data types,\n",
    "    and stripping whitespace from column names and string fields.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the data from the file\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return None\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Could not parse the file '{file_path}'. Please ensure it's a valid CSV.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading the data: {e}\")\n",
    "        return None\n",
    "\n",
    "    df['Shipping Date']=df['Shipped Date']\n",
    "    df = df.drop(\"Shipped Date\", axis = 1)\n",
    "    # Display initial columns and data types\n",
    "    print(\"Initial Columns:\", df.columns.tolist())\n",
    "    print(\"Initial Data Types:\\n\", df.dtypes)\n",
    "    \n",
    "    # Strip whitespace from all column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Replace 'N/A' strings with NaN for proper handling\n",
    "    df.replace('N/A', pd.NA, inplace=True)\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    categorical_cols = [col for col in categorical_cols if col != 'ID']  # Exclude 'ID' if present\n",
    "\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # Convert 'Date Created' and 'Shipping Date' to datetime, coercing errors to NaT\n",
    "    for date_col in ['Date Created', 'Shipping Date']:\n",
    "        if date_col in df.columns:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        else:\n",
    "            print(f\"Warning: '{date_col}' column not found in the data.\")\n",
    "\n",
    "    # Convert quantity columns to numeric, coercing errors to NaN\n",
    "    quantity_cols = ['Order Quantity', 'Shipped Quantity', 'Unshipped Quantity']\n",
    "    for qty_col in quantity_cols:\n",
    "        if qty_col in df.columns:\n",
    "            df[qty_col] = pd.to_numeric(df[qty_col], errors='coerce')\n",
    "        else:\n",
    "            print(f\"Warning: '{qty_col}' column not found in the data.\")\n",
    "\n",
    "    # Handle missing values\n",
    "    # For categorical columns, we will leave NaN as is (do not fill with 'No')\n",
    "    # For numerical columns, fill NaN with the median of each column\n",
    "    df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n",
    "\n",
    "    # Ensure 'ID' is treated as a string to avoid numeric issues\n",
    "    if 'ID' in df.columns:\n",
    "        df['ID'] = df['ID'].astype(str)\n",
    "\n",
    "    # Drop 'Data Type' column if present to avoid serialization issues\n",
    "    if 'Data Type' in df.columns:\n",
    "        df = df.drop('Data Type', axis=1)\n",
    "        print(\"Column 'Data Type' was found and has been excluded from the analysis.\")\n",
    "\n",
    "    # Strip whitespace from specific columns if they contain string values\n",
    "    for col in quantity_cols:\n",
    "        if col in df.columns and df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "    # Display data types after preprocessing\n",
    "    print(\"Data Types After Preprocessing:\\n\", df.dtypes)\n",
    "#     print(\"First 5 Rows After Preprocessing:\\n\", df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------- Aggregation Logic -------------------------- #\n",
    "\n",
    "def aggregate_orders(df):\n",
    "    \"\"\"\n",
    "    Aggregates the orders by 'Customer Reference' and 'Club Name'. \n",
    "    Ensures that if any item within a group has a status of 'OPEN', the entire group's status is set to 'OPEN'.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The preprocessed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Aggregated DataFrame.\n",
    "    \"\"\"\n",
    "    def aggregate_status(status_series):\n",
    "        \"\"\"\n",
    "        Aggregates the 'Sales Order Header Status'.\n",
    "        If any status in the group is 'OPEN', return 'OPEN'.\n",
    "        Otherwise, return the most common status or another logic as needed.\n",
    "        \"\"\"\n",
    "        if 'OPEN' in status_series.values:\n",
    "            return 'OPEN'\n",
    "        else:\n",
    "            # If there are multiple statuses and 'OPEN' is not present, define your own logic\n",
    "            # For example, return the mode (most frequent status)\n",
    "            return status_series.mode().iloc[0] if not status_series.mode().empty else 'SHIPPED'\n",
    "\n",
    "    # Perform the aggregation\n",
    "    grouped_df = df.groupby(['Customer Reference', 'Club Name']).agg({\n",
    "        'Date Created': 'min',                        # Earliest Date Created\n",
    "        'Order Quantity': 'sum',                      # Total Order Quantity\n",
    "        'Shipped Quantity': 'sum',                    # Total Shipped Quantity\n",
    "        'Unshipped Quantity': 'sum',                  # Total Unshipped Quantity\n",
    "        'Shipping Date': 'max',                       # Latest Shipping Date\n",
    "        'Sales Order Header Status': aggregate_status  # Custom aggregation for status\n",
    "#         'Home Long Sleeve': 'max',\n",
    "#         'Away Long Sleeve': 'max'\n",
    "        \n",
    "    }).reset_index()\n",
    "\n",
    "    print(\"Aggregated Data Types:\\n\", grouped_df.dtypes)\n",
    "    print(\"First 5 Rows of Aggregated Data:\\n\", grouped_df.head())\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "# -------------------------- Main Execution -------------------------- #\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    input_file = 'shippingdates/Rush Soccer 5.4 - Details.csv'  # Replace with the actual file path\n",
    "    output_file = 'shippingdates/aggregated_orders5.4.csv'      # Desired output file name\n",
    "\n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data(input_file)\n",
    "    if df is None:\n",
    "        return  # Exit if data loading failed\n",
    "\n",
    "    # Aggregate orders\n",
    "    aggregated_df = aggregate_orders(df)\n",
    "\n",
    "    # Save the aggregated data to a new CSV file\n",
    "    try:\n",
    "        aggregated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Aggregated data saved to '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the aggregated data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4df5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d675b275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Players who purchased long sleeve jerseys:\n",
      "\n",
      "Summary Statistics (Unique Players by Long Sleeve Status):\n",
      "Long Sleeve Status\n",
      "Did Not Purchase Long Sleeve           55\n",
      "Purchased Both Long Sleeves (2)         8\n",
      "Purchased Only Away Long Sleeve (1)     5\n",
      "Purchased Only Home Long Sleeve (1)     1\n",
      "dtype: int64\n",
      "\n",
      "Total Home Long Sleeve Jerseys Purchased: 9\n",
      "Total Away Long Sleeve Jerseys Purchased: 13\n",
      "\n",
      "Files saved: '5.4Order_Summary_with_ls_status.csv' and 'Rush Soccer 5.4 - Details_with_ls_status.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame.\n",
    "df = pd.read_csv(\"shippingdates/Rush Soccer 5.4 - Rush Details Category 2025.csv\")\n",
    "\n",
    "# OPTIONAL: To restrict analysis to a specific club, e.g., \"Rush Montana\", uncomment:\n",
    "df = df[df[\"Club Name\"] == \"Rush Nevada\"]\n",
    "df = df[df[\"Category\"].isin([\"Field Players Mandatory Kit\", \"Goalkeepers Mandatory Kit\",\"Competitive Items\"])]\n",
    "\n",
    "# Convert \"Order Date\" to datetime and filter for orders in 2025.\n",
    "df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"], errors=\"coerce\")\n",
    "df = df[df[\"Order Date\"].dt.year == 2025]\n",
    "\n",
    "# Define the long sleeve product descriptions.\n",
    "home_ls = \"NEVADA RUSH BROOKLYN II RUSH SOCCER PYRAMIDS LONG SLEEVE MATCH JERSEY PROMO BLUE BLACK WHITE\"\n",
    "away_ls = \"NEVADA RUSH BROOKLYN II RUSH SOCCER SCATTERED SHARDE LONG SLEEVE MATCH JERSEY WHITE PROMO GREY BLACK\"\n",
    "\n",
    "# Function to compute long sleeve status and counts from a list of product names.\n",
    "def compute_ls_status_player(product_names):\n",
    "    # Convert the list into a pandas Series for easier aggregation.\n",
    "    s = pd.Series(product_names)\n",
    "    count_home = s.isin([home_ls]).sum()\n",
    "    count_away = s.isin([away_ls]).sum()\n",
    "    total = count_home + count_away\n",
    "    if total == 0:\n",
    "        status = \"Did Not Purchase Long Sleeve\"\n",
    "    elif count_home > 0 and count_away > 0:\n",
    "        status = f\"Purchased Both Long Sleeves ({total})\"\n",
    "    elif count_home > 0:\n",
    "        status = f\"Purchased Only Home Long Sleeve ({count_home})\"\n",
    "    elif count_away > 0:\n",
    "        status = f\"Purchased Only Away Long Sleeve ({count_away})\"\n",
    "    else:\n",
    "        status = \"Did Not Purchase Long Sleeve\"\n",
    "    return status, count_home, count_away\n",
    "\n",
    "# Group by \"Player ID\" so that if a player purchases in multiple orders, they are aggregated together.\n",
    "grouped = df.groupby(\"Player ID\").agg({\n",
    "    \"Order ID\": lambda x: \" ; \".join(x.astype(str).unique()),\n",
    "    \"Order Date\": \"min\",             # earliest order date for that player\n",
    "    \"Customer Email\": \"first\",       # assuming one email per player\n",
    "    \"Club Name\": \"first\",\n",
    "    \"Player Name\": \"first\",\n",
    "    \"Product Name\": lambda x: list(x)  # aggregate all product names into a list\n",
    "}).reset_index()\n",
    "\n",
    "# Compute long sleeve status and counts for each player.\n",
    "grouped[[\"Long Sleeve Status\", \"Home Count\", \"Away Count\"]] = grouped[\"Product Name\"].apply(\n",
    "    lambda prod_list: pd.Series(compute_ls_status_player(prod_list))\n",
    ")\n",
    "\n",
    "# Print out unique players (Player ID, Customer Email, Player Name, and status) for players who purchased any long sleeve.\n",
    "purchased_ls = grouped[grouped[\"Long Sleeve Status\"] != \"Did Not Purchase Long Sleeve\"]\n",
    "print(\"Players who purchased long sleeve jerseys:\")\n",
    "# print(purchased_ls[[\"Player ID\", \"Customer Email\", \"Player Name\", \"Long Sleeve Status\", \"Home Count\", \"Away Count\"]])\n",
    "\n",
    "# Print summary statistics: count of unique players by Long Sleeve Status.\n",
    "summary = grouped.groupby(\"Long Sleeve Status\").size()\n",
    "print(\"\\nSummary Statistics (Unique Players by Long Sleeve Status):\")\n",
    "print(summary)\n",
    "\n",
    "# Print total counts of Home and Away long sleeve jerseys purchased across all players.\n",
    "total_home = grouped[\"Home Count\"].sum()\n",
    "total_away = grouped[\"Away Count\"].sum()\n",
    "print(f\"\\nTotal Home Long Sleeve Jerseys Purchased: {total_home}\")\n",
    "print(f\"Total Away Long Sleeve Jerseys Purchased: {total_away}\")\n",
    "\n",
    "# Write the aggregated order summary to CSV in the specified directory.\n",
    "grouped.to_csv(\"LongSleeveOrders/5.4Order_Summary_with_ls_status.csv\", index=False)\n",
    "\n",
    "# Optionally, merge the summary information back into the original DataFrame by Player ID.\n",
    "df = df.merge(grouped[[\"Player ID\", \"Long Sleeve Status\", \"Home Count\", \"Away Count\"]],\n",
    "              on=\"Player ID\", how=\"left\")\n",
    "df.to_csv(\"LongSleeveOrders/Rush Soccer 5.4 - Details_with_ls_status.csv\", index=False)\n",
    "\n",
    "print(\"\\nFiles saved: '5.4Order_Summary_with_ls_status.csv' and 'Rush Soccer 5.4 - Details_with_ls_status.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5d948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b8341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25ccb361",
   "metadata": {},
   "source": [
    "Below is a section of a script that will allow you to determine the amount of packages shipped per month given the details sheet from the Capelli report Details tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "490ef52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # packages_shipped_analysis.ipynb\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # -------------------------- Step 1: Load the Data -------------------------- #\n",
    "\n",
    "# # Define the file path\n",
    "# file_path = 'shippingdates/Rush Soccer 11.24.24 - Details.csv'  # Replace with your actual file path\n",
    "\n",
    "# # Load the CSV file into a pandas DataFrame\n",
    "# # Assuming the CSV is tab-separated based on the sample data\n",
    "# try:\n",
    "#     df = pd.read_csv(file_path, sep='\\t', engine='python')\n",
    "#     print(\"Data loaded successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading data: {e}\")\n",
    "\n",
    "# # -------------------------- Step 2: Inspect the Data -------------------------- #\n",
    "\n",
    "# # Display the first few rows to understand the data structure\n",
    "# print(\"First 5 rows of the dataset:\")\n",
    "# print(df.head())\n",
    "\n",
    "# # Display the column names and their data types\n",
    "# print(\"\\nColumn Names:\")\n",
    "# print(df.columns)\n",
    "# print(\"\\nData Types:\")\n",
    "# print(df.dtypes)\n",
    "\n",
    "# # -------------------------- Step 3: Clean Column Names -------------------------- #\n",
    "\n",
    "# # Strip leading and trailing whitespace from all column names\n",
    "# df.columns = df.columns.str.strip()\n",
    "\n",
    "# # Verify column names after stripping\n",
    "# print(\"\\nCleaned Column Names:\")\n",
    "# print(df.columns)\n",
    "\n",
    "# # -------------------------- Step 4: Clean Specific Columns -------------------------- #\n",
    "\n",
    "# # Define columns that may contain whitespace and need to be stripped\n",
    "# columns_to_strip = ['Order Quantity', 'Shipped Quantity', 'Unshipped Quantity']\n",
    "\n",
    "# # Strip whitespace from these columns if they are of object type (strings)\n",
    "# df[columns_to_strip] = df[columns_to_strip].apply(\n",
    "#     lambda x: x.str.strip() if x.dtype == \"object\" else x\n",
    "# )\n",
    "\n",
    "# # Convert 'Shipped Quantity' and 'Unshipped Quantity' to numeric types\n",
    "# # Replace non-numeric entries with 0\n",
    "# df['Shipped Quantity'] = pd.to_numeric(df['Shipped Quantity'], errors='coerce').fillna(0).astype(int)\n",
    "# df['Unshipped Quantity'] = pd.to_numeric(df['Unshipped Quantity'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# # Verify the changes\n",
    "# print(\"\\nData Types After Conversion:\")\n",
    "# print(df.dtypes)\n",
    "\n",
    "# # -------------------------- Step 5: Convert Date Columns -------------------------- #\n",
    "\n",
    "# # Convert 'Date Created' and 'Shipping Date' to datetime format\n",
    "# # Coerce errors to NaT (Not a Time) for invalid dates\n",
    "# df['Date Created'] = pd.to_datetime(df['Date Created'], errors='coerce', format='%m/%d/%Y')\n",
    "# df['Shipping Date'] = pd.to_datetime(df['Shipping Date'], errors='coerce', format='%m/%d/%Y')\n",
    "\n",
    "# # Verify the conversion\n",
    "# print(\"\\nDate Columns After Conversion:\")\n",
    "# print(df[['Date Created', 'Shipping Date']].head())\n",
    "\n",
    "# # -------------------------- Step 6: Handle Invalid 'Shipping Date' Entries -------------------------- #\n",
    "\n",
    "# # Identify rows with invalid 'Shipping Date' (NaT)\n",
    "# invalid_shipping_dates = df['Shipping Date'].isna()\n",
    "# print(f\"\\nNumber of rows with invalid 'Shipping Date': {invalid_shipping_dates.sum()}\")\n",
    "\n",
    "# # Option 1: Remove rows with invalid 'Shipping Date'\n",
    "# df_clean = df.dropna(subset=['Shipping Date']).copy()\n",
    "# print(f\"Number of rows after removing invalid 'Shipping Date': {df_clean.shape[0]}\")\n",
    "\n",
    "# # Optionally, you can choose to fill invalid 'Shipping Date' with 'Date Created' or another default date\n",
    "# # Uncomment the following lines if you prefer this approach\n",
    "# # df['Shipping Date'] = df['Shipping Date'].fillna(df['Date Created'])\n",
    "# # df_clean = df.dropna(subset=['Shipping Date']).copy()\n",
    "\n",
    "# # -------------------------- Step 7: Extract Month-Year from 'Shipping Date' -------------------------- #\n",
    "\n",
    "# # Create a new column 'Month-Year' in 'MMMM YYYY' format (e.g., July 2024)\n",
    "# df_clean['Month-Year'] = df_clean['Shipping Date'].dt.strftime('%B %Y')\n",
    "\n",
    "# # Verify the new column\n",
    "# print(\"\\nSample 'Month-Year' Entries:\")\n",
    "# print(df_clean[['Shipping Date', 'Month-Year']].head())\n",
    "\n",
    "# # -------------------------- Step 8: Remove Duplicate Tracking Numbers -------------------------- #\n",
    "\n",
    "# # Assuming each 'Tracking Number' uniquely identifies a package, remove duplicates\n",
    "# # If 'Tracking Number' is not unique per package, adjust accordingly\n",
    "# df_unique = df_clean.drop_duplicates(subset=['Tracking Number'])\n",
    "\n",
    "# # Verify the removal of duplicates\n",
    "# print(f\"\\nNumber of unique packages after removing duplicates: {df_unique.shape[0]}\")\n",
    "\n",
    "# # -------------------------- Step 9: Group by 'Month-Year' and Count Unique Packages -------------------------- #\n",
    "\n",
    "# # Group by 'Month-Year' and count unique 'Tracking Number' to get the number of packages shipped each month\n",
    "# packages_shipped = df_unique.groupby('Month-Year')['Tracking Number'].nunique().reset_index()\n",
    "\n",
    "# # Rename the columns for clarity\n",
    "# packages_shipped.columns = ['Month-Year', 'Unique Packages Shipped']\n",
    "\n",
    "# # -------------------------- Step 10: Sort the Results Chronologically -------------------------- #\n",
    "\n",
    "# # Convert 'Month-Year' back to datetime for sorting\n",
    "# packages_shipped['Month-Year-Date'] = pd.to_datetime(packages_shipped['Month-Year'], format='%B %Y')\n",
    "\n",
    "# # Sort by the new datetime column\n",
    "# packages_shipped = packages_shipped.sort_values('Month-Year-Date')\n",
    "\n",
    "# # Drop the auxiliary datetime column\n",
    "# packages_shipped = packages_shipped.drop('Month-Year-Date', axis=1)\n",
    "\n",
    "# # -------------------------- Step 11: Display the Results -------------------------- #\n",
    "\n",
    "# print(\"\\nNumber of Packages Shipped Each Month:\")\n",
    "# print(packages_shipped)\n",
    "\n",
    "# # -------------------------- Step 12: (Optional) Save the Results to CSV -------------------------- #\n",
    "\n",
    "# # Define the output file path\n",
    "# output_file = 'packages_shipped_per_month.csv'\n",
    "\n",
    "# # Save the DataFrame to a new CSV file\n",
    "# packages_shipped.to_csv(output_file, index=False)\n",
    "# print(f\"\\nAggregated data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd848b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39340ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5892f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
